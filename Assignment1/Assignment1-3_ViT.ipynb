{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning Assignment #1<br> Part 1-3. Training Vision Transformers (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jaehoon Lee, September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PDF file.**\n",
    "\n",
    "Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **PyTorch**. <br>\n",
    "In this notebook, you will learn to understand and build the basic components of Vision Tranformer(ViT). Then, you will try to classify images in the FashionMNIST datatset and explore the effects of different components of ViTs.\n",
    "<br>\n",
    "There are **2 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results. \n",
    "\n",
    "### Some helpful tutorials and references for assignment #1-2:\n",
    "- [1] Pytorch official documentation. [[link]](https://pytorch.org/docs/stable/index.html)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Alexey Dosovitskiy et al., \"An Image is Worth 16 x 16 Words: Transformers for Image Recognition at Scale\", ICLR 2021. [[pdf]](https://arxiv.org/pdf/2010.11929.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Vision Transformer\n",
    "Here, you will build the basic components of Vision Transformer(ViT). <br>\n",
    "\n",
    "![Vision Transformer](imgs/ViT.png)\n",
    "\n",
    "Using the explanation and code provided as guidance, <br>\n",
    "Define each component of ViT. <br>\n",
    "\n",
    "\n",
    "#### ViT architecture:\n",
    "* ViT model consists with input patch embedding, positional embeddings, transformer encoder, etc.\n",
    "* Patch embedding\n",
    "* Positional embeddings\n",
    "* Transformer encoder with\n",
    "    * Attention module\n",
    "    * MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Patch Embed\n",
    "\n",
    "**Initialization**: When you create an instance of the PatchEmbedding class, you specify the image_size, patch_size, and in_channels. image_size is the height and width of the input image, patch_size is the size of each patch, and in_channels is the number of input image channels (e.g., 3 for RGB images). \n",
    "\n",
    "**Convolutional Projection**: Inside the PatchEmbedding class, a 2D convolutional layer (nn.Conv2d) is used to perform a patch-based projection. This convolutional layer has a kernel size of patch_size, which defines the size of each patch, and a stride of patch_size, which ensures that patches do not overlap. The convolutional layer effectively extracts image patches.\n",
    "\n",
    "**Reshaping**: After the convolutional projection, the output tensor is reshaped using view. It is transformed from a 4D tensor with dimensions (batch_size, in_channels, H, W) to a 3D tensor with dimensions (batch_size, num_patches, patch_dim). num_patches is the total number of non-overlapping patches in the image, and patch_dim is the number of output channels from the convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        x = self.projection(x)\n",
    "        x = x.view(x.size(0), -1, x.size(1))\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x # output dimension must be: (batch size, number of patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention\n",
    "\n",
    "**Initialization**\n",
    "* dim: The input dimension of the sequence. This is the dimensionality of the queries, keys, and values.\n",
    "* num_heads: The number of attention heads to use. Multi-head attention allows the model to focus on different parts of the input simultaneously.\n",
    "\n",
    "**Linear Projections (qkv and proj)**: The qkv linear layer takes the input sequence and projects it into three parts: queries (q), keys (k), and values (v). The output of this layer has a shape of (batch_size, sequence_length, 3 * dim).\n",
    "\n",
    "**Forward Pass (forward method)**: In the forward pass, the input tensor x is processed through the attention mechanism. Here's what happens:<br>\n",
    "* The linear projection qkv is applied to x, producing a tensor of shape (batch_size, sequence_length, 3 * dim).|\n",
    "* This tensor is reshaped to have dimensions (batch_size, sequence_length, 3, num_heads, head_dim). The permute operation rearranges the dimensions to (3, batch_size, num_heads, sequence_length, head_dim), making it suitable for multi-head attention.\n",
    "* The three parts, q, k, and v, are extracted from the reshaped tensor.\n",
    "* The attention scores are computed by taking the dot product of queries q and keys k. The result is scaled by self.scale.\n",
    "* The attention scores are passed through a softmax activation along the last dimension (sequence_length), producing attention weights.\n",
    "* The weighted sum of values v is computed using the attention weights.\n",
    "* The result is transposed and reshaped to its original shape, and then passed through the proj linear layer.\n",
    "* The final output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x # output dimension must be: (batch size, number of patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP\n",
    "\n",
    "The MLP module must consist of three layers:\n",
    "* fully conncted layer 1\n",
    "* activation layer\n",
    "* fully conncted layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x # output dimension must be: (batch size, number of patches, out_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformer Block\n",
    "The transformer block contains the attention module and MLP module which have residual connections. \n",
    "Refer to the following image and build the forward pass.\n",
    "\n",
    "![Transformer Block](imgs/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vision Transformer\n",
    "\n",
    "Using all the components that you built above, **complete** the vision transformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=28, patch_size=4, in_chans=1, num_classes=10, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., norm_layer=nn.LayerNorm, ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ############################################################################## \n",
    "        # similarly to cls_token, define a learnable positional embedding that matches the patchified input token size.\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,  norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        #                           IMPLEMENT YOUR CODE                              #\n",
    "        ##############################################################################\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Concatenate class tokens to patch embedding\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        # Add positional embedding to patches\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Forward through encoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Use class token for classification\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        \n",
    "        # Classifier head\n",
    "        x = self.head(x)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                              END YOUR CODE                                 #\n",
    "        ##############################################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a small ViT model on FashionMNIST dataset.\n",
    "\n",
    "Define and Train a vision transformer on FashionMNIST dataset. **(You must reach above 85% for full points.)** <br>\n",
    "Train with at least 5 different hyperparameter settings varying the following ViT hyperparameters. \n",
    "Report the setting for the best performance.\n",
    "\n",
    "#### ViT hyperparameters:\n",
    "* patch_size\n",
    "* embed_dim\n",
    "* depth\n",
    "* num_heads\n",
    "* mlp_ratio\n",
    "* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3060 Ti)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [00:24<11:50, 24.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 loss: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 2/30 [00:47<11:04, 23.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 loss: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 3/30 [01:11<10:36, 23.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 loss: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 4/30 [01:34<10:10, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 loss: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 5/30 [01:57<09:47, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 loss: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 6/30 [02:21<09:23, 23.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 7/30 [02:44<08:59, 23.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 8/30 [03:08<08:35, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 loss: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 9/30 [03:31<08:12, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 10/30 [03:55<07:48, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 loss: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [04:18<07:24, 23.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 loss: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 12/30 [04:41<07:01, 23.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 loss: 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 13/30 [05:05<06:37, 23.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 loss: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 14/30 [05:28<06:14, 23.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 loss: 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 15/30 [05:51<05:50, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 loss: 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 16/30 [06:15<05:27, 23.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 loss: 0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 17/30 [06:38<05:03, 23.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 loss: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 18/30 [07:02<04:40, 23.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 loss: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 19/30 [07:25<04:17, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 loss: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 20/30 [07:48<03:53, 23.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 loss: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [08:12<03:30, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 loss: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 22/30 [08:35<03:06, 23.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 loss: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 23/30 [08:58<02:43, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 loss: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 24/30 [09:22<02:20, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 loss: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 25/30 [09:45<01:56, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 loss: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 26/30 [10:08<01:33, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 loss: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 27/30 [10:32<01:10, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 loss: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 28/30 [10:55<00:46, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 loss: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 29/30 [11:19<00:23, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 loss: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [11:42<00:00, 23.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 loss: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:01<00:00, 55.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.34\n",
      "Test accuracy: 87.41%\n",
      "Saved Trained Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Train():\n",
    "    ##############################################################################\n",
    "    #                           IMPLEMENT YOUR CODE                              #\n",
    "    ##############################################################################\n",
    "\n",
    "    patch_size=4\n",
    "    embed_dim=256\n",
    "    depth=6\n",
    "    num_heads=8 # make sure embed_dim is divisible by num_heads!\n",
    "    mlp_ratio=4\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                              END YOUR CODE                                 #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # Loading data\n",
    "    transform = ToTensor()\n",
    "\n",
    "    train_set = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_set = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n",
    "    # Defining model and training options\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    \n",
    "    model = VisionTransformer(patch_size=patch_size, embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio).to(device)\n",
    "    model_path = './vit.pth'\n",
    "    N_EPOCHS = 30\n",
    "    LR = 0.005\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('Saved Trained Model.')\n",
    "    \n",
    "Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did and discovered here\n",
    "In this cell you should write all the settings tried and performances you obtained. Report what you did and what you discovered from the trials.\n",
    "You can write in Korean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관찰 내용\n",
    "- Hidden dimension과 head의 수를 늘리는 것보다 depth를 늘리는 것이 연산량을 덜 증가시키면서 학습을 더 빠르게 하는 데 더 효과적이다.\n",
    "- Hyperparameter를 적절히 설정하면 모델의 복잡도를 낮게 유지하면서(학습에 드는 자원이 적으면서) 빠르게 학습시킬 수 있다. 어떻게 보면 적절한 d_vc를 찾는 것 같기도 하다.\n",
    "\n",
    "### 실험 1\n",
    "#### Hyperparams\n",
    "```\n",
    "patch_size=4\n",
    "embed_dim=256\n",
    "depth=6\n",
    "num_heads=8 # make sure embed_dim is divisible by num_heads!\n",
    "mlp_ratio=4\n",
    "N_EPOCHS = 10\n",
    "LR = 0.005\n",
    "```\n",
    "#### 결과 분석\n",
    "```\n",
    "Using device:  cuda (NVIDIA GeForce RTX 3060 Ti)\n",
    "Epoch 1/10 loss: 0.80\n",
    "Epoch 2/10 loss: 0.55\n",
    "Epoch 3/10 loss: 0.48\n",
    "Epoch 4/10 loss: 0.48\n",
    "Epoch 5/10 loss: 0.49\n",
    "Epoch 6/10 loss: 0.44\n",
    "Epoch 7/10 loss: 0.41\n",
    "Epoch 8/10 loss: 0.39\n",
    "Epoch 9/10 loss: 0.38\n",
    "Epoch 10/10 loss: 0.37\n",
    "Test loss: 0.40\n",
    "Test accuracy: 85.75%\n",
    "```\n",
    "\n",
    "최종적으로는 기준 정확도를 넘겼으나, 중간중간 loss가 다시 증가하는 모습도 보임.\n",
    "\n",
    "이에 learning rate를 조금 줄여서 학습시켜보기로 함.\n",
    "\n",
    "### 실험 2\n",
    "#### Hyperparams\n",
    "```\n",
    "patch_size=4\n",
    "embed_dim=256\n",
    "depth=6\n",
    "num_heads=8 # make sure embed_dim is divisible by num_heads!\n",
    "mlp_ratio=4\n",
    "N_EPOCHS = 10\n",
    "LR = 0.002\n",
    "```\n",
    "#### 결과 분석\n",
    "```\n",
    "Using device:  cuda (NVIDIA GeForce RTX 3060 Ti)\n",
    "Epoch 1/10 loss: 0.64\n",
    "Epoch 2/10 loss: 0.43\n",
    "Epoch 3/10 loss: 0.39\n",
    "Epoch 4/10 loss: 0.39\n",
    "Epoch 5/10 loss: 0.37\n",
    "Epoch 6/10 loss: 0.36\n",
    "Epoch 7/10 loss: 0.35\n",
    "Epoch 8/10 loss: 0.34\n",
    "Epoch 9/10 loss: 0.35\n",
    "Epoch 10/10 loss: 0.38\n",
    "Test loss: 0.41\n",
    "Test accuracy: 85.35%\n",
    "```\n",
    "학습 과정에서 이전보다 훨씬 빠른 속도로 loss가 감소하는 모습을 보임. 하지만 test accuracy는 이전보다 증가하였음.\n",
    "\n",
    "이에 learning rate를 이전 값으로 되돌리고, 모델의 depth와 epoch의 수를 늘려보기로 함.\n",
    "\n",
    "### 실험 3\n",
    "#### Hyperparams\n",
    "```\n",
    "patch_size=4\n",
    "embed_dim=256\n",
    "depth=8\n",
    "num_heads=8 # make sure embed_dim is divisible by num_heads!\n",
    "mlp_ratio=4\n",
    "N_EPOCHS = 15\n",
    "LR = 0.005\n",
    "```\n",
    "#### 결과 분석\n",
    "```\n",
    "Using device:  cuda (NVIDIA GeForce RTX 3060 Ti)\n",
    "Epoch 1/15 loss: 0.94\n",
    "Epoch 2/15 loss: 0.53\n",
    "Epoch 3/15 loss: 0.48\n",
    "Epoch 4/15 loss: 0.46\n",
    "Epoch 5/15 loss: 0.46\n",
    "Epoch 6/15 loss: 0.43\n",
    "Epoch 7/15 loss: 0.41\n",
    "Epoch 8/15 loss: 0.40\n",
    "Epoch 9/15 loss: 0.40\n",
    "Epoch 10/15 loss: 0.42\n",
    "Epoch 11/15 loss: 0.39\n",
    "Epoch 12/15 loss: 0.37\n",
    "Epoch 13/15 loss: 0.36\n",
    "Epoch 14/15 loss: 0.36\n",
    "Epoch 15/15 loss: 0.37\n",
    "Test loss: 0.40\n",
    "Test accuracy: 85.19%\n",
    "```\n",
    "학습을 시작할 때의 loss는 이전보다 높았지만, 학습이 충분히 진행된 후의 loss는 이전과 비슷했다.\n",
    "\n",
    "반대로 매우 단순한 모델을 학습시켜보기로 했다.\n",
    "\n",
    "### 실험 4\n",
    "#### Hyperparams\n",
    "```\n",
    "patch_size=4\n",
    "embed_dim=128\n",
    "depth=4\n",
    "num_heads=2 # make sure embed_dim is divisible by num_heads!\n",
    "mlp_ratio=4\n",
    "N_EPOCHS = 20\n",
    "LR = 0.005\n",
    "```\n",
    "#### 결과 분석\n",
    "```\n",
    "Using device:  cuda (NVIDIA GeForce RTX 3060 Ti)\n",
    "Epoch 1/20 loss: 0.67\n",
    "Epoch 2/20 loss: 0.46\n",
    "Epoch 3/20 loss: 0.42\n",
    "Epoch 4/20 loss: 0.41\n",
    "Epoch 5/20 loss: 0.39\n",
    "Epoch 6/20 loss: 0.39\n",
    "Epoch 7/20 loss: 0.39\n",
    "Epoch 8/20 loss: 0.38\n",
    "Epoch 9/20 loss: 0.36\n",
    "Epoch 10/20 loss: 0.36\n",
    "Epoch 11/20 loss: 0.35\n",
    "Epoch 12/20 loss: 0.35\n",
    "Epoch 13/20 loss: 0.35\n",
    "Epoch 14/20 loss: 0.35\n",
    "Epoch 15/20 loss: 0.33\n",
    "Epoch 16/20 loss: 0.33\n",
    "Epoch 17/20 loss: 0.34\n",
    "Epoch 18/20 loss: 0.32\n",
    "Epoch 19/20 loss: 0.37\n",
    "Epoch 20/20 loss: 0.33\n",
    "Testing: 100%|██████████| 79/79 [00:00<00:00, 130.63it/s]\n",
    "Test loss: 0.35\n",
    "Test accuracy: 87.19%\n",
    "```\n",
    "훨씬 성능이 좋았다. 마지막으로는 모델의 크기를 조금만 늘리고 학습을 꽤 오래 진행해보기로 했다.\n",
    "\n",
    "### 실험 5\n",
    "#### Hyperparams\n",
    "```\n",
    "patch_size=4\n",
    "embed_dim=256\n",
    "depth=6\n",
    "num_heads=8 # make sure embed_dim is divisible by num_heads!\n",
    "mlp_ratio=4\n",
    "N_EPOCHS = 30\n",
    "LR = 0.005\n",
    "```\n",
    "#### 결과 분석\n",
    "```\n",
    "Using device:  cuda (NVIDIA GeForce RTX 3060 Ti)\n",
    "Epoch 1/30 loss: 0.92\n",
    "Epoch 2/30 loss: 0.61\n",
    "Epoch 3/30 loss: 0.54\n",
    "Epoch 4/30 loss: 0.50\n",
    "Epoch 5/30 loss: 0.45\n",
    "Epoch 6/30 loss: 0.44\n",
    "Epoch 7/30 loss: 0.42\n",
    "Epoch 8/30 loss: 0.42\n",
    "Epoch 9/30 loss: 0.39\n",
    "Epoch 10/30 loss: 0.39\n",
    "Epoch 11/30 loss: 0.44\n",
    "Epoch 12/30 loss: 0.40\n",
    "Epoch 13/30 loss: 0.37\n",
    "Epoch 14/30 loss: 0.36\n",
    "Epoch 15/30 loss: 0.35\n",
    "Epoch 16/30 loss: 0.34\n",
    "Epoch 17/30 loss: 0.33\n",
    "Epoch 18/30 loss: 0.32\n",
    "Epoch 19/30 loss: 0.32\n",
    "Epoch 20/30 loss: 0.31\n",
    "Epoch 21/30 loss: 0.31\n",
    "Epoch 22/30 loss: 0.31\n",
    "Epoch 23/30 loss: 0.30\n",
    "Epoch 24/30 loss: 0.29\n",
    "Epoch 25/30 loss: 0.29\n",
    "Epoch 26/30 loss: 0.29\n",
    "Epoch 27/30 loss: 0.28\n",
    "Epoch 28/30 loss: 0.27\n",
    "Epoch 29/30 loss: 0.27\n",
    "Epoch 30/30 loss: 0.26\n",
    "Testing: 100%|██████████| 79/79 [00:01<00:00, 55.51it/s]\n",
    "Test loss: 0.34\n",
    "Test accuracy: 87.41%\n",
    "```\n",
    "이전보다 아주 약간 성능이 증가하였다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
