{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2280e3",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning Term Project <br> Part 1. Prompt Engineering for Text-to-Image Generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623dbcd",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them.\n",
    "\n",
    "HuggingFace ðŸ¤— provides highly abstracted classes by bundling various models implemented in different ways into a single class, making it easy to use.\n",
    "\n",
    "In this material, you can check the similarity between images using the CLIP feature space.\n",
    "\n",
    "**DO NOT clear the final outputs so that TAs can grade both your code and results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415337d7",
   "metadata": {},
   "source": [
    "Prepare the CLIP model and processor using the pre-trained checkpoint in HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "model_ckpt = 'google/vit-base-patch16-224-in21k'\n",
    "processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d2de1",
   "metadata": {},
   "source": [
    "Obtain given images' 1D feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1_given = './data/image1_given.png' # Do not modify.\n",
    "path2_given = './data/image2_given.png' # Do not modify.\n",
    "image1_given = Image.open(path1_given).convert('RGB')\n",
    "image2_given = Image.open(path2_given).convert('RGB')\n",
    "inputs = processor(images=[image1_given, image2_given], return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "feat_given = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ce257",
   "metadata": {},
   "source": [
    "Put your generated images in ./data/ folder.  \n",
    "**Your final generated images should be saved as ./data/image1_gen.png, ./data/image2_gen.png.**  \n",
    "Then, obtain generated images' 1D feature.\n",
    "\n",
    "Now, we give examples of generated images (image1_gen_provided.png, image2_gen_provided.png) to show how this material works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You should comment out these lines when you submit. ###\n",
    "path1_gen = './data/image1_gen_provided.png'\n",
    "path2_gen = './data/image2_gen_provided.png'\n",
    "\n",
    "### You should use these lines when you submit. ###\n",
    "# path1_gen = './data/image1_gen.png'\n",
    "# path2_gen = './data/image2_gen.png'\n",
    "\n",
    "image1_gen = Image.open(path1_gen).convert('RGB')\n",
    "image2_gen = Image.open(path2_gen).convert('RGB')\n",
    "inputs = processor(images=[image1_gen, image2_gen], return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "feat_gen = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96b937",
   "metadata": {},
   "source": [
    "Visualization: 1st column: given images vs 2nd column: generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35301ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    w, h = w//2, h//2\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.resize((w, h))\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid([image1_given, image1_gen, image2_given, image2_gen], 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd8709",
   "metadata": {},
   "source": [
    "Compute similarities between given and generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd162e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = F.cosine_similarity(feat_given, feat_gen)\n",
    "print(f'The similarities between given and generated features are')\n",
    "print(f'Image1: {sim[0].item():.4f}, Image2: {sim[1].item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65d01d",
   "metadata": {},
   "source": [
    "# Write your TTI models and prompts\n",
    "\n",
    "## Example)\n",
    "Your model: StableDiffusion 2.0\n",
    "\n",
    "Your prompt: A photo of a man running in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb9892",
   "metadata": {},
   "source": [
    "## 1st Image\n",
    "> Your model :\n",
    "\n",
    "> Your prompt:\n",
    "\n",
    "## 2nd Image\n",
    "> Your model :\n",
    "\n",
    "> Your prompt:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-23]",
   "language": "python",
   "name": "conda-env-deep-learning-23-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
