import torch
import transformers


def load_model(model_name_or_path, cache_dir='cache'):
    '''
    Load model and corresponding tokenizer
    
    tokenizer = 
    model = 
    '''
    
    return tokenizer, model